{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Quanghoaai/Podo/blob/main/YouTube_to_chatbot_notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Updated Code for Article Processing with Secure API Key Input"
      ],
      "metadata": {
        "id": "T5rtzX0E9Eks"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZC0Jt-Lb9Ekt"
      },
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "!pip install -qU \\\n",
        "    pinecone-client[grpc]==2.2.1 \\\n",
        "    langchain==0.0.162 \\\n",
        "    tiktoken==0.4.0 \\\n",
        "    datasets==2.12.0 \\\n",
        "    youtube_transcript_api \\\n",
        "    google-api-python-client \\\n",
        "    google-generativeai\n",
        "\n",
        "# Import libraries\n",
        "import os\n",
        "import google.generativeai as genai\n",
        "import tiktoken\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "from urllib.parse import urlparse, parse_qs\n",
        "from youtube_transcript_api import YouTubeTranscriptApi\n",
        "\n",
        "# Extract video ID from a YouTube URL\n",
        "def extract_video_id_from_url(video_url):\n",
        "    \"\"\"\n",
        "    Extracts the video ID from a YouTube video URL.\n",
        "    Example: https://www.youtube.com/watch?v=dQw4w9WgXcQ -> 'dQw4w9WgXcQ'\n",
        "    \"\"\"\n",
        "    parsed_url = urlparse(video_url)\n",
        "    if parsed_url.hostname == 'youtu.be':  # Shortened URL format\n",
        "        return parsed_url.path[1:]  # Remove the leading '/'\n",
        "    elif parsed_url.hostname in ('www.youtube.com', 'youtube.com'):\n",
        "        if parsed_url.path == '/watch':\n",
        "            return parse_qs(parsed_url.query)['v'][0]\n",
        "        elif parsed_url.path.startswith('/embed/'):  # Embedded video URL\n",
        "            return parsed_url.path.split('/')[2]\n",
        "    return None\n",
        "\n",
        "# Get transcript for a specific video in multiple languages\n",
        "def get_transcript_for_video(video_id, languages=['vi', 'en']):\n",
        "    transcripts = {}\n",
        "    for lang in languages:\n",
        "        try:\n",
        "            transcript = YouTubeTranscriptApi.get_transcript(video_id, languages=[lang])\n",
        "            transcripts[lang] = transcript\n",
        "            print(f\"Successfully fetched transcript for language: {lang}\")\n",
        "        except Exception as ex:\n",
        "            print(f\"No transcript available for language: {lang} [{ex}]\")\n",
        "            transcripts[lang] = []\n",
        "    return transcripts\n",
        "\n",
        "# Combine transcripts from multiple languages into one text\n",
        "def combine_transcripts(transcripts):\n",
        "    combined_text = \"\"\n",
        "    for lang, transcript in transcripts.items():\n",
        "        combined_text += f\"\\n--- Language: {lang.upper()} ---\\n\"\n",
        "        for item in transcript:\n",
        "            combined_text += item['text'] + '\\n'\n",
        "    return combined_text\n",
        "\n",
        "# Write transcript to file\n",
        "def write_to_file(text, output_file=\"YouTube.txt\"):\n",
        "    with open(output_file, 'w') as f:\n",
        "        f.write(text)\n",
        "\n",
        "# Load text from file\n",
        "def load_text(file_path):\n",
        "    with Path(file_path).open(\"r\") as file:\n",
        "        return file.read()\n",
        "\n",
        "# Save responses to file\n",
        "def save_to_file(responses, output_file):\n",
        "    with Path(output_file).open('w') as file:\n",
        "        file.write(\"\\n\".join(responses))\n",
        "\n",
        "# Call Gemini API to clean text\n",
        "def call_gemini_api(chunk, model):\n",
        "    \"\"\"Calls the Gemini API to clean the given chunk of text.\"\"\"\n",
        "    prompt = f\"\"\"Clean the following transcripts of all grammatical mistakes, misplaced words, and identify the speakers:\n",
        "    {chunk}\n",
        "    \"\"\"\n",
        "    try:\n",
        "        response = model.generate_content(prompt)\n",
        "        response.resolve()  # Ensure the response is ready\n",
        "        return response.text.strip()\n",
        "    except Exception as e:\n",
        "        print(f\"Error during Gemini API call: {e}\")\n",
        "        return \"\"  # Handle errors gracefully\n",
        "\n",
        "# Split text into chunks\n",
        "def split_into_chunks(text, n_tokens=300):\n",
        "    encoding = tiktoken.encoding_for_model('gpt-3.5-turbo')  # Using gpt-3.5-turbo tokenizer for token estimation\n",
        "    tokens = encoding.encode(text)\n",
        "    chunks = []\n",
        "    for i in range(0, len(tokens), n_tokens):\n",
        "        chunk_tokens = tokens[i:i + n_tokens]\n",
        "        try:\n",
        "            chunks.append(encoding.decode(chunk_tokens))\n",
        "        except Exception as e:\n",
        "            print(f\"Error decoding tokens: {e}\")\n",
        "            continue\n",
        "    return chunks\n",
        "\n",
        "# Process chunks through Gemini API\n",
        "def process_chunks(input_file, output_file, model, delay=0):  # delay in seconds (if you hit a rate limit error)\n",
        "    text = load_text(input_file)\n",
        "    chunks = split_into_chunks(text)[:5]\n",
        "    responses = []\n",
        "    for chunk in tqdm(chunks):\n",
        "        responses.append(call_gemini_api(chunk, model))\n",
        "    save_to_file(responses, output_file)\n",
        "\n",
        "# Clean user query\n",
        "def clean_query(query, model):\n",
        "    prompt = f\"\"\"Clean the following query to make sure it is grammatically correct and can retrieve the correct information: {query}\"\"\"\n",
        "    try:\n",
        "        response = model.generate_content(prompt)\n",
        "        response.resolve()  # Ensure the response is ready\n",
        "        return response.text.strip()\n",
        "    except Exception as e:\n",
        "        print(f\"Error during Gemini API call: {e}\")\n",
        "        return \"\"  # Handle errors gracefully\n",
        "\n",
        "# Query knowledge base\n",
        "def query_knowledge_base(query, knowledge_base, model):\n",
        "    \"\"\"\n",
        "    Queries the knowledge base using the Gemini API.\n",
        "    \"\"\"\n",
        "    context = \"\\n\".join(knowledge_base)\n",
        "    prompt = f\"\"\"You are a chatbot that answers questions to the best of your ability using the context provided.\n",
        "    Context:\n",
        "    {context}\n",
        "    Question: {query}\n",
        "    \"\"\"\n",
        "    try:\n",
        "        response = model.generate_content(prompt)\n",
        "        response.resolve()  # Ensure the response is ready\n",
        "        return response.text.strip()\n",
        "    except Exception as e:\n",
        "        print(f\"Error during Gemini API call: {e}\")\n",
        "        return \"\"  # Handle errors gracefully\n",
        "\n",
        "# Conversational agent\n",
        "def conversational_agent(query, knowledge_base, chat_history, model):\n",
        "    \"\"\"\n",
        "    A conversational agent that uses the Gemini API to answer questions.\n",
        "    \"\"\"\n",
        "    # Clean query before processing\n",
        "    cleaned_query = clean_query(query, model)\n",
        "    # Query the knowledge base\n",
        "    response = query_knowledge_base(cleaned_query, knowledge_base, model)\n",
        "    # Update the chat history\n",
        "    chat_history.append(f\"User: {query}\")\n",
        "    chat_history.append(f\"Assistant: {response}\")\n",
        "    return response, chat_history\n",
        "\n",
        "# Main function\n",
        "if __name__ == \"__main__\":\n",
        "    # Step 1: Request API key from the user\n",
        "    GOOGLE_API_KEY = input(\"Please enter your Gemini API key: \").strip()\n",
        "    if not GOOGLE_API_KEY:\n",
        "        print(\"API key is required to proceed.\")\n",
        "        exit()\n",
        "\n",
        "    # Configure Gemini API with the provided key\n",
        "    genai.configure(api_key=GOOGLE_API_KEY)\n",
        "    model = genai.GenerativeModel('gemini-1.5-pro-latest')  # Or another suitable Gemini model\n",
        "\n",
        "    # Step 2: Provide a YouTube video URL\n",
        "    video_url = input(\"Please enter the YouTube video URL: \").strip()\n",
        "    video_id = extract_video_id_from_url(video_url)\n",
        "\n",
        "    if not video_id:\n",
        "        print(\"Invalid YouTube URL\")\n",
        "    else:\n",
        "        print(f\"Extracted Video ID: {video_id}\")\n",
        "\n",
        "        # Step 3: Get transcript in Vietnamese and English\n",
        "        transcripts = get_transcript_for_video(video_id, languages=['vi', 'en'])\n",
        "\n",
        "        # Step 4: Combine transcripts into one text\n",
        "        combined_text = combine_transcripts(transcripts)\n",
        "\n",
        "        # Step 5: Write combined transcript to file\n",
        "        write_to_file(combined_text, \"YouTube.txt\")\n",
        "        print(\"Combined transcript has been written to YouTube.txt\")\n",
        "\n",
        "        # Step 6: Process transcript through Gemini API\n",
        "        input_file = \"YouTube.txt\"\n",
        "        output_file = \"clean_transcript.txt\"\n",
        "        process_chunks(input_file, output_file, model)\n",
        "\n",
        "        # Step 7: Build knowledge base\n",
        "        with Path('clean_transcript.txt').open('r') as file:\n",
        "            lines = file.read().splitlines()\n",
        "        chunks = [' '.join(lines[i:i+5]) for i in range(0, len(lines), 5)]\n",
        "\n",
        "        # Step 8: Use conversational agent\n",
        "        knowledge_base = chunks.copy()\n",
        "        chat_history = []\n",
        "\n",
        "        while True:\n",
        "            query = input(\"Ask a question (or type 'exit' to quit): \").strip()\n",
        "            if query.lower() == 'exit':\n",
        "                break\n",
        "\n",
        "            response, chat_history = conversational_agent(query, knowledge_base, chat_history, model)\n",
        "            print(response)\n",
        "\n",
        "        print(chat_history)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.Full Code for Multi-Language Transcript (Vietnamese and English)\n",
        "python\n"
      ],
      "metadata": {
        "id": "qYeI0E4bzS4y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install dependencies\n",
        "!pip install -qU \\\n",
        "    pinecone-client[grpc]==2.2.1 \\\n",
        "    langchain==0.0.162 \\\n",
        "    tiktoken==0.4.0 \\\n",
        "    datasets==2.12.0 \\\n",
        "    youtube_transcript_api \\\n",
        "    google-api-python-client \\\n",
        "    google-generativeai\n",
        "\n",
        "# Import libraries\n",
        "import os\n",
        "import google.generativeai as genai\n",
        "import tiktoken\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "from urllib.parse import urlparse, parse_qs\n",
        "from youtube_transcript_api import YouTubeTranscriptApi\n",
        "\n",
        "# Extract video ID from a YouTube URL\n",
        "def extract_video_id_from_url(video_url):\n",
        "    \"\"\"\n",
        "    Extracts the video ID from a YouTube video URL.\n",
        "    Example: https://www.youtube.com/watch?v=dQw4w9WgXcQ -> 'dQw4w9WgXcQ'\n",
        "    \"\"\"\n",
        "    parsed_url = urlparse(video_url)\n",
        "    if parsed_url.hostname == 'youtu.be':  # Shortened URL format\n",
        "        return parsed_url.path[1:]  # Remove the leading '/'\n",
        "    elif parsed_url.hostname in ('www.youtube.com', 'youtube.com'):\n",
        "        if parsed_url.path == '/watch':\n",
        "            return parse_qs(parsed_url.query)['v'][0]\n",
        "        elif parsed_url.path.startswith('/embed/'):  # Embedded video URL\n",
        "            return parsed_url.path.split('/')[2]\n",
        "    return None\n",
        "\n",
        "# Get transcript for a specific video in multiple languages\n",
        "def get_transcript_for_video(video_id, languages=['vi', 'en']):\n",
        "    transcripts = {}\n",
        "    for lang in languages:\n",
        "        try:\n",
        "            transcript = YouTubeTranscriptApi.get_transcript(video_id, languages=[lang])\n",
        "            transcripts[lang] = transcript\n",
        "            print(f\"Successfully fetched transcript for language: {lang}\")\n",
        "        except Exception as ex:\n",
        "            print(f\"No transcript available for language: {lang} [{ex}]\")\n",
        "            transcripts[lang] = []\n",
        "    return transcripts\n",
        "\n",
        "# Combine transcripts from multiple languages into one text\n",
        "def combine_transcripts(transcripts):\n",
        "    combined_text = \"\"\n",
        "    for lang, transcript in transcripts.items():\n",
        "        combined_text += f\"\\n--- Language: {lang.upper()} ---\\n\"\n",
        "        for item in transcript:\n",
        "            combined_text += item['text'] + '\\n'\n",
        "    return combined_text\n",
        "\n",
        "# Write transcript to file\n",
        "def write_to_file(text, output_file=\"YouTube.txt\"):\n",
        "    with open(output_file, 'w') as f:\n",
        "        f.write(text)\n",
        "\n",
        "# Load text from file\n",
        "def load_text(file_path):\n",
        "    with Path(file_path).open(\"r\") as file:\n",
        "        return file.read()\n",
        "\n",
        "# Save responses to file\n",
        "def save_to_file(responses, output_file):\n",
        "    with Path(output_file).open('w') as file:\n",
        "        file.write(\"\\n\".join(responses))\n",
        "\n",
        "# Call Gemini API to clean text\n",
        "def call_gemini_api(chunk, model):\n",
        "    \"\"\"Calls the Gemini API to clean the given chunk of text.\"\"\"\n",
        "    prompt = f\"\"\"Clean the following transcripts of all grammatical mistakes, misplaced words, and identify the speakers:\n",
        "    {chunk}\n",
        "    \"\"\"\n",
        "    try:\n",
        "        response = model.generate_content(prompt)\n",
        "        response.resolve()  # Ensure the response is ready\n",
        "        return response.text.strip()\n",
        "    except Exception as e:\n",
        "        print(f\"Error during Gemini API call: {e}\")\n",
        "        return \"\"  # Handle errors gracefully\n",
        "\n",
        "# Split text into chunks\n",
        "def split_into_chunks(text, n_tokens=300):\n",
        "    encoding = tiktoken.encoding_for_model('gpt-3.5-turbo')  # Using gpt-3.5-turbo tokenizer for token estimation\n",
        "    tokens = encoding.encode(text)\n",
        "    chunks = []\n",
        "    for i in range(0, len(tokens), n_tokens):\n",
        "        chunk_tokens = tokens[i:i + n_tokens]\n",
        "        try:\n",
        "            chunks.append(encoding.decode(chunk_tokens))\n",
        "        except Exception as e:\n",
        "            print(f\"Error decoding tokens: {e}\")\n",
        "            continue\n",
        "    return chunks\n",
        "\n",
        "# Process chunks through Gemini API\n",
        "def process_chunks(input_file, output_file, model, delay=0):  # delay in seconds (if you hit a rate limit error)\n",
        "    text = load_text(input_file)\n",
        "    chunks = split_into_chunks(text)[:5]\n",
        "    responses = []\n",
        "    for chunk in tqdm(chunks):\n",
        "        responses.append(call_gemini_api(chunk, model))\n",
        "    save_to_file(responses, output_file)\n",
        "\n",
        "# Clean user query\n",
        "def clean_query(query, model):\n",
        "    prompt = f\"\"\"Clean the following query to make sure it is grammatically correct and can retrieve the correct information: {query}\"\"\"\n",
        "    try:\n",
        "        response = model.generate_content(prompt)\n",
        "        response.resolve()  # Ensure the response is ready\n",
        "        return response.text.strip()\n",
        "    except Exception as e:\n",
        "        print(f\"Error during Gemini API call: {e}\")\n",
        "        return \"\"  # Handle errors gracefully\n",
        "\n",
        "# Query knowledge base\n",
        "def query_knowledge_base(query, knowledge_base, model):\n",
        "    \"\"\"\n",
        "    Queries the knowledge base using the Gemini API.\n",
        "    \"\"\"\n",
        "    context = \"\\n\".join(knowledge_base)\n",
        "    prompt = f\"\"\"You are a chatbot that answers questions to the best of your ability using the context provided.\n",
        "    Context:\n",
        "    {context}\n",
        "    Question: {query}\n",
        "    \"\"\"\n",
        "    try:\n",
        "        response = model.generate_content(prompt)\n",
        "        response.resolve()  # Ensure the response is ready\n",
        "        return response.text.strip()\n",
        "    except Exception as e:\n",
        "        print(f\"Error during Gemini API call: {e}\")\n",
        "        return \"\"  # Handle errors gracefully\n",
        "\n",
        "# Conversational agent\n",
        "def conversational_agent(query, knowledge_base, chat_history, model):\n",
        "    \"\"\"\n",
        "    A conversational agent that uses the Gemini API to answer questions.\n",
        "    \"\"\"\n",
        "    # Clean query before processing\n",
        "    cleaned_query = clean_query(query, model)\n",
        "    # Query the knowledge base\n",
        "    response = query_knowledge_base(cleaned_query, knowledge_base, model)\n",
        "    # Update the chat history\n",
        "    chat_history.append(f\"User: {query}\")\n",
        "    chat_history.append(f\"Assistant: {response}\")\n",
        "    return response, chat_history\n",
        "\n",
        "# Main function\n",
        "if __name__ == \"__main__\":\n",
        "    # Step 1: Request API key from the user\n",
        "    GOOGLE_API_KEY = input(\"Please enter your Gemini API key: \").strip()\n",
        "    if not GOOGLE_API_KEY:\n",
        "        print(\"API key is required to proceed.\")\n",
        "        exit()\n",
        "\n",
        "    # Configure Gemini API with the provided key\n",
        "    genai.configure(api_key=GOOGLE_API_KEY)\n",
        "    model = genai.GenerativeModel('gemini-1.5-pro-latest')  # Or another suitable Gemini model\n",
        "\n",
        "    # Step 2: Provide a YouTube video URL\n",
        "    video_url = input(\"Please enter the YouTube video URL: \").strip()\n",
        "    video_id = extract_video_id_from_url(video_url)\n",
        "\n",
        "    if not video_id:\n",
        "        print(\"Invalid YouTube URL\")\n",
        "    else:\n",
        "        print(f\"Extracted Video ID: {video_id}\")\n",
        "\n",
        "        # Step 3: Get transcript in Vietnamese and English\n",
        "        transcripts = get_transcript_for_video(video_id, languages=['vi', 'en'])\n",
        "\n",
        "        # Step 4: Combine transcripts into one text\n",
        "        combined_text = combine_transcripts(transcripts)\n",
        "\n",
        "        # Step 5: Write combined transcript to file\n",
        "        write_to_file(combined_text, \"YouTube.txt\")\n",
        "        print(\"Combined transcript has been written to YouTube.txt\")\n",
        "\n",
        "        # Step 6: Process transcript through Gemini API\n",
        "        input_file = \"YouTube.txt\"\n",
        "        output_file = \"clean_transcript.txt\"\n",
        "        process_chunks(input_file, output_file, model)\n",
        "\n",
        "        # Step 7: Build knowledge base\n",
        "        with Path('clean_transcript.txt').open('r') as file:\n",
        "            lines = file.read().splitlines()\n",
        "        chunks = [' '.join(lines[i:i+5]) for i in range(0, len(lines), 5)]\n",
        "\n",
        "        # Step 8: Use conversational agent\n",
        "        knowledge_base = chunks.copy()\n",
        "        chat_history = []\n",
        "\n",
        "        while True:\n",
        "            query = input(\"Ask a question (or type 'exit' to quit): \").strip()\n",
        "            if query.lower() == 'exit':\n",
        "                break\n",
        "\n",
        "            response, chat_history = conversational_agent(query, knowledge_base, chat_history, model)\n",
        "            print(response)\n",
        "\n",
        "        print(chat_history)"
      ],
      "metadata": {
        "id": "KXA_yK2ByJvG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "kDDX8ExO8-lF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install dependencies"
      ],
      "metadata": {
        "id": "2FIzZ73bR5yr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pva9ehKXUpU2"
      },
      "outputs": [],
      "source": [
        "!pip install -qU \\\n",
        "    openai==0.27.7 \\\n",
        "    pinecone-client[grpc]==2.2.1 \\\n",
        "    langchain==0.0.162 \\\n",
        "    tiktoken==0.4.0 \\\n",
        "    datasets==2.12.0 \\\n",
        "    youtube_transcript_api"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Scrape an entire YouTube channel into one .txt file"
      ],
      "metadata": {
        "id": "YZ7U_sF7VlS_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install google-api-python-client\n",
        "import googleapiclient.discovery\n",
        "from tqdm import tqdm\n",
        "from youtube_transcript_api import YouTubeTranscriptApi\n",
        "api_key = \"\" #@param {type:\"string\"}\n",
        "channel_id = \"\" #@param {type:\"string\"} # Get your channel ID here https://commentpicker.com/youtube-channel-id.php\n",
        "\n",
        "def get_channel_videos(channel_id, api_key):\n",
        "    youtube = googleapiclient.discovery.build(\n",
        "        \"youtube\", \"v3\", developerKey=api_key)\n",
        "\n",
        "    video_ids = []\n",
        "    page_token = None\n",
        "\n",
        "    while True:\n",
        "        request = youtube.search().list(\n",
        "            part=\"snippet\",\n",
        "            channelId=channel_id,\n",
        "            maxResults=50,  # Fetch 50 videos at a time\n",
        "            pageToken=page_token  # Add pagination\n",
        "        )\n",
        "        response = request.execute()\n",
        "\n",
        "        video_ids += [item['id']['videoId'] for item in response['items'] if item['id']['kind'] == 'youtube#video']\n",
        "\n",
        "        # Check if there are more videos to fetch\n",
        "        if 'nextPageToken' in response:\n",
        "            page_token = response['nextPageToken']\n",
        "        else:\n",
        "            break\n",
        "\n",
        "    return video_ids\n",
        "\n",
        "def get_transcripts(video_ids):\n",
        "    transcripts = []\n",
        "    for video_id in tqdm(video_ids):\n",
        "        try:\n",
        "            transcript = YouTubeTranscriptApi.get_transcript(video_id)\n",
        "            transcripts.append(transcript)\n",
        "        except Exception as ex:\n",
        "            print(f\"An error occurred for video: {video_id} [{ex}]\")\n",
        "    return transcripts\n",
        "\n",
        "def write_to_file(transcripts):\n",
        "    with open('YouTube.txt', 'w') as f:\n",
        "        for transcript in transcripts:\n",
        "            for item in transcript:\n",
        "                f.write(item['text'] + '\\n')\n",
        "\n",
        "def main(api_key, channel_id):\n",
        "    video_ids = get_channel_videos(channel_id, api_key)[:20]\n",
        "    transcripts = get_transcripts(video_ids)\n",
        "    print(transcripts)\n",
        "    write_to_file(transcripts)\n",
        "\n",
        "main(api_key, channel_id)"
      ],
      "metadata": {
        "id": "FqE3b7L6VtR6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run our text file through infiniteGPT to clean its grammar and punctuation."
      ],
      "metadata": {
        "id": "ZukPFl65V-lm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "YOUR_OPENAI_API_KEY = \"\" #@param {type:\"string\"}\n",
        "import openai\n",
        "from langchain.chat_models.openai import ChatOpenAI\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "import tiktoken\n",
        "from pathlib import Path\n",
        "from langchain.schema import (\n",
        "    HumanMessage,\n",
        "    SystemMessage\n",
        ")\n",
        "\n",
        "chat = ChatOpenAI(model=\"gpt-3.5-turbo\",temperature=0.2,max_tokens=500)\n",
        "\n",
        "openai.api_key = YOUR_OPENAI_API_KEY\n",
        "\n",
        "def load_text(file_path):\n",
        "    with Path(file_path).open(\"r\") as file:\n",
        "        return file.read()\n",
        "\n",
        "def save_to_file(responses, output_file):\n",
        "    with Path(output_file).open('w') as file:\n",
        "        file.write(\"\\n\".join(responses))\n",
        "\n",
        "def call_openai_api(chunk):\n",
        "    messages = [\n",
        "        SystemMessage(content=\"Clean the following transcripts of all gramatical mistakes, misplaced words, and identify the speakers.\"),\n",
        "        HumanMessage(content=chunk)\n",
        "    ]\n",
        "    response = chat(messages)\n",
        "    return response.content.strip()\n",
        "\n",
        "def split_into_chunks(text, n_tokens=300):\n",
        "    encoding = tiktoken.encoding_for_model('gpt-3.5-turbo')\n",
        "    tokens = encoding.encode(text)\n",
        "    chunks = []\n",
        "    for i in range(0, len(tokens), n_tokens):\n",
        "        chunks.append(' '.join(encoding.decode(tokens[i:i + n_tokens])))\n",
        "    return chunks\n",
        "\n",
        "def process_chunks(input_file, output_file, delay=0):  # delay in seconds (if you hit a rate limit error)\n",
        "    text = load_text(input_file)\n",
        "    chunks = split_into_chunks(text)[:5]\n",
        "    responses = []\n",
        "    for chunk in tqdm(chunks):\n",
        "        responses.append(call_openai_api(chunk))\n",
        "\n",
        "    save_to_file(responses, output_file)\n",
        "\n",
        "    # Specify your input and output files\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    input_file = \"YouTube.txt\"\n",
        "    output_file = \"clean_transcript.txt\"\n",
        "    process_chunks(input_file, output_file)\n",
        "\n",
        "    # Can take up to a few minutes to run depending on the size of your data input"
      ],
      "metadata": {
        "id": "mrGqeGjUWJW5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZTgrOQziXUto"
      },
      "source": [
        "## Building the Knowledge Base"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qNyRsz0ZXXaq"
      },
      "source": [
        "We start by constructing our knowledge base. Make sure to input the correct .txt file. For most users, this will be your cleaned_transcript.txt file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "laSDMjqQXuj-"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Read the .txt file into Python\n",
        "with Path('clean_transcript.txt').open('r') as file:\n",
        "    lines = file.read().splitlines()\n",
        "\n",
        "# Group the lines into chunks of 5\n",
        "chunks = [' '.join(lines[i:i+5]) for i in range(0, len(lines), 5)]\n",
        "\n",
        "# Convert list of chunks into a DataFrame\n",
        "data = pd.DataFrame(chunks, columns=['context'])\n",
        "\n",
        "# Add an index column and a name column\n",
        "data['name'] = 'youtube'\n",
        "\n",
        "# Remove duplicates (if any)\n",
        "data.drop_duplicates(subset='context', keep='first', inplace=True)\n",
        "\n",
        "print(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B2_Pt7N6Zg2X"
      },
      "source": [
        "### Initialize the Embedding Model and Vector DB"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bGoS84KYZnSK"
      },
      "source": [
        "We'll be using OpenAI's `text-embedding-ada-002` model initialize via LangChain and the Pinecone vector DB. We start by initializing the embedding model, for this we need an [OpenAI API key](https://platform.openai.com/).\n",
        "\n",
        "*(Note that OpenAI is a paid service and so running the remainder of this notebook will cost a few dimes)*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U57x2_87YSpb"
      },
      "outputs": [],
      "source": [
        "from getpass import getpass\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "\n",
        "OPENAI_API_KEY = getpass(\"OpenAI API Key: \")  # platform.openai.com\n",
        "model_name = 'text-embedding-ada-002'\n",
        "\n",
        "embed = OpenAIEmbeddings(\n",
        "    model=model_name,\n",
        "    openai_api_key=OPENAI_API_KEY\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQTfOTR6aBRS"
      },
      "source": [
        "Next we initialize the vector database. For this we need a [free API key](https://app.pinecone.io/), then we create the index:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C3wrG-9yaJel"
      },
      "outputs": [],
      "source": [
        " import pinecone\n",
        "\n",
        "# find API key in console at app.pinecone.io\n",
        "\n",
        "YOUR_API_KEY = '' #@param {type:\"string\"}\n",
        "# find ENV (cloud region) next to API key in console\n",
        "YOUR_ENV = '' #@param {type:\"string\"}\n",
        "\n",
        "index_name = 'youtube-chatbot-agent'\n",
        "pinecone.init(\n",
        "    api_key=YOUR_API_KEY,\n",
        "    environment=YOUR_ENV\n",
        ")\n",
        "\n",
        "if index_name not in pinecone.list_indexes():\n",
        "    # we create a new index\n",
        "    pinecone.create_index(\n",
        "        name=index_name,\n",
        "        metric='dotproduct',\n",
        "        dimension=1536  # 1536 dim of text-embedding-ada-002\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uiSWrAQ5aRco"
      },
      "source": [
        "Then connect to the index:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bfsfuFmqaS4G"
      },
      "outputs": [],
      "source": [
        "index = pinecone.GRPCIndex(index_name)\n",
        "index.describe_index_stats()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AD5IGOoLaVx7"
      },
      "source": [
        "We should see that the new Pinecone index has a `total_vector_count` of `0`, as we haven't added any vectors yet.\n",
        "\n",
        "## Indexing\n",
        "\n",
        "We can perform the indexing task using the Pinecone python client directly. We will do this in batches of `100` or more."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "9eea814462bb48f2ac468e649311019b",
            "53bfc84e8a4c43f98095e6dcc543f85a",
            "4ea0d00cccae46ebbe5043d5910667e9",
            "ca5c4ed971e94b4aa0dd8b8abc61363a",
            "985bf8b6b856453eb49218880723344b",
            "0c7f76f9ecce49baa310f1acff5450a9",
            "ac9aff5b622848f99528d8b1ef39d1f7",
            "5604993df40848f49b4f39c2f9e3855c",
            "8c169a1c70854b07bbcbe5fcbb508429",
            "000f3fb5bce64e5980d53a64ec814d73",
            "ee17cdf174f440459ab0dfdd20e109b3"
          ]
        },
        "id": "AhDcbRGTaWPi",
        "outputId": "eec92834-8ceb-486e-f666-bccd3a1e218d"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/9 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9eea814462bb48f2ac468e649311019b"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "from tqdm.auto import tqdm\n",
        "from uuid import uuid4\n",
        "\n",
        "# Reset index and ensure 'index' column is added\n",
        "data = data.reset_index(drop=True)\n",
        "data = data.reset_index()\n",
        "\n",
        "batch_size = 100\n",
        "\n",
        "for i in tqdm(range(0, len(data), batch_size)):\n",
        "    # get end of batch\n",
        "    i_end = min(len(data), i+batch_size)\n",
        "    batch = data.iloc[i:i_end]\n",
        "\n",
        "    # first get metadata fields for this record\n",
        "    metadatas = [{\n",
        "      'text': record[0],  # 'text' will contain the same data as 'context'\n",
        "      'name': record[1]\n",
        "    } for record in batch.itertuples(index=False)]\n",
        "\n",
        "    # get the list of contexts / documents\n",
        "    documents = batch['context'].tolist()\n",
        "\n",
        "    # create document embeddings\n",
        "    embeds = embed.embed_documents(documents)\n",
        "\n",
        "    # get IDs and convert them to strings\n",
        "    ids = batch['index'].astype(str).tolist()\n",
        "\n",
        "    # add everything to pinecone\n",
        "    index.upsert(vectors=list(zip(ids, embeds, metadatas)))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jDUnLdy1b7G1"
      },
      "source": [
        "We've indexed everything, now we can check the number of vectors in our index like so:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SiccGZKAb_Qo"
      },
      "outputs": [],
      "source": [
        "index.describe_index_stats()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-3oolT5cCR8"
      },
      "source": [
        "## Creating a Vector Store and Querying"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DcZ12U06cCH5"
      },
      "source": [
        "Now that we've build our index we can switch back over to LangChain. We start by initializing a vector store using the same index we just built. We do that like so:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0MBJ477-cFNw"
      },
      "outputs": [],
      "source": [
        "from langchain.vectorstores import Pinecone\n",
        "\n",
        "text_field = \"text\"\n",
        "\n",
        "# switch back to normal index for langchain\n",
        "index = pinecone.Index(index_name)\n",
        "\n",
        "vectorstore = Pinecone(\n",
        "    index, embed.embed_query, text_field\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3K3xRthWcXzW"
      },
      "source": [
        "As in previous examples, we can use the `similarity_search` method to do a pure semantic search (without the generation component)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uITMZtzschJF"
      },
      "outputs": [],
      "source": [
        "query = \"how to make viral videos?\"\n",
        "\n",
        "vectorstore.similarity_search(\n",
        "    query,  # our search query\n",
        "    k=3  # return 3 most relevant docs\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-zGF6YsgczqT"
      },
      "source": [
        "Looks like we're getting good results. Let's take a look at how we can begin integrating this into a conversational agent."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tFsIOm73dcOI"
      },
      "source": [
        "## Initializing the Conversational Agent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XMv6TXWkdfNR"
      },
      "source": [
        "Our conversational agent needs a Chat LLM, conversational memory, and a `RetrievalQA` chain to initialize. We create these using:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zMRs9Klic5-Y"
      },
      "outputs": [],
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.chains.conversation.memory import ConversationBufferWindowMemory\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "# chat completion llm\n",
        "llm = ChatOpenAI(\n",
        "    openai_api_key=OPENAI_API_KEY,\n",
        "    model_name='gpt-3.5-turbo',\n",
        "    temperature=0.0\n",
        ")\n",
        "# conversational memory\n",
        "conversational_memory = ConversationBufferWindowMemory(\n",
        "    memory_key='chat_history',\n",
        "    k=5,\n",
        "    return_messages=True\n",
        ")\n",
        "# retrieval qa chain\n",
        "qa = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=vectorstore.as_retriever()\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ySfWyZLdboX"
      },
      "source": [
        "Using these we can generate an answer using the `run` method:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LaYSq0V-dxHw"
      },
      "outputs": [],
      "source": [
        "qa.run(query)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DtSXR5RXdyU0"
      },
      "source": [
        "But this isn't yet ready for our conversational agent. For that we need to convert this retrieval chain into a tool. We do that like so:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FwCYrS4duqBW"
      },
      "outputs": [],
      "source": [
        "from langchain.agents import Tool\n",
        "\n",
        "tools = [\n",
        "    Tool(\n",
        "        name='Knowledge Base',\n",
        "        func=qa.run,\n",
        "        description=(\n",
        "            'use this tool for every query to get '\n",
        "            'more information and stories on the topic'\n",
        "        )\n",
        "    )\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wXi_0ipTvM_l"
      },
      "source": [
        "Now we can initialize the agent like so:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JaKTzPUEvOoy"
      },
      "outputs": [],
      "source": [
        "from langchain.agents import initialize_agent\n",
        "\n",
        "agent = initialize_agent(\n",
        "    agent='chat-conversational-react-description',\n",
        "    tools=tools,\n",
        "    llm=llm,\n",
        "    verbose=True,\n",
        "    max_iterations=3,\n",
        "    early_stopping_method='generate',\n",
        "    memory=conversational_memory\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WbXl-AzVvszB"
      },
      "source": [
        "With that our retrieval augmented conversational agent is ready and we can begin using it. For better results, make the sys_msg description as specific and elaborate as possible."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "agent.agent.llm_chain.prompt"
      ],
      "metadata": {
        "id": "Qjh1aZ_ppzPe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sys_msg = \"\"\"You are [NAME], a [description] & content creator. You are an expert in [description]. Answer the user's questions with [desired tone].\n",
        "\"\"\"\n",
        "\n",
        "prompt = agent.agent.create_prompt(\n",
        "    system_message=sys_msg,\n",
        "    tools=tools\n",
        ")\n",
        "agent.agent.llm_chain.prompt = prompt"
      ],
      "metadata": {
        "id": "7G_Djcu-qBKy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent.agent.llm_chain.prompt"
      ],
      "metadata": {
        "id": "KRgxFTkuqUQ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IlxUBWKcvzeP"
      },
      "source": [
        "### Using the Conversational Agent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZapCP4Pv2kz"
      },
      "source": [
        "To make queries we simply call the `agent` directly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RJoAhy76vzAB"
      },
      "outputs": [],
      "source": [
        "agent(\"Tell me about yourself\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PWivmw9F3bCw"
      },
      "source": [
        "We're also able to ask questions that refer to previous interactions in the conversation and the agent is able to refer to the conversation history to as a source of information.\n",
        "\n",
        "Once finished, delete the Pinecone index to save resources:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pa1whr8V3Wfm"
      },
      "outputs": [],
      "source": [
        "pinecone.delete_index(index_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ykg5TYA033yR"
      },
      "source": [
        "---"
      ]
    },
    {
      "source": [
        "# Install dependencies Hòa test\n",
        "!pip install -qU \\\n",
        "    pinecone-client[grpc]==2.2.1 \\\n",
        "    langchain==0.0.162 \\\n",
        "    tiktoken==0.4.0 \\\n",
        "    datasets==2.12.0 \\\n",
        "    youtube_transcript_api \\\n",
        "    google-api-python-client \\\n",
        "    google-generativeai\n",
        "\n",
        "# Scrape an entire YouTube channel into one .txt file\n",
        "\n",
        "import googleapiclient.discovery\n",
        "from tqdm import tqdm\n",
        "from youtube_transcript_api import YouTubeTranscriptApi\n",
        "# Make sure to replace with your actual API key\n",
        "api_key = \"\"  #@param {type:\"string\"}\n",
        "# Replace with the actual channel ID\n",
        "channel_id = \"\"  #@param {type:\"string\"}\n",
        "\n",
        "def get_channel_videos(channel_id, api_key):\n",
        "    youtube = googleapiclient.discovery.build(\n",
        "        \"youtube\", \"v3\", developerKey=api_key)\n",
        "\n",
        "    video_ids = []\n",
        "    page_token = None\n",
        "\n",
        "    while True:\n",
        "        request = youtube.search().list(\n",
        "            part=\"snippet\",\n",
        "            channelId=channel_id,\n",
        "            maxResults=50,  # Fetch 50 videos at a time\n",
        "            pageToken=page_token  # Add pagination\n",
        "        )\n",
        "        response = request.execute()\n",
        "\n",
        "        video_ids += [item['id']['videoId'] for item in response['items'] if item['id']['kind'] == 'youtube#video']\n",
        "\n",
        "        # Check if there are more videos to fetch\n",
        "        if 'nextPageToken' in response:\n",
        "            page_token = response['nextPageToken']\n",
        "        else:\n",
        "            break\n",
        "\n",
        "    return video_ids\n",
        "\n",
        "def get_transcripts(video_ids):\n",
        "    transcripts = []\n",
        "    for video_id in tqdm(video_ids):\n",
        "        try:\n",
        "            transcript = YouTubeTranscriptApi.get_transcript(video_id)\n",
        "            transcripts.append(transcript)\n",
        "        except Exception as ex:\n",
        "            print(f\"An error occurred for video: {video_id} [{ex}]\")\n",
        "    return transcripts\n",
        "\n",
        "def write_to_file(transcripts):\n",
        "    with open('YouTube.txt', 'w') as f:\n",
        "        for transcript in transcripts:\n",
        "            for item in transcript:\n",
        "                f.write(item['text'] + '\\n')\n",
        "\n",
        "def main(api_key, channel_id):\n",
        "    video_ids = get_channel_videos(channel_id, api_key)[:20]\n",
        "    transcripts = get_transcripts(video_ids)\n",
        "    print(transcripts)\n",
        "    write_to_file(transcripts)\n",
        "\n",
        "main(api_key, channel_id)\n",
        "\n",
        "### Run our text file through infiniteGPT to clean its grammar and punctuation.\n",
        "import os\n",
        "import google.generativeai as genai\n",
        "import tiktoken\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Set your Gemini API key.  You can also set it as an environment variable.\n",
        "# Make sure to replace with your actual API key\n",
        "GOOGLE_API_KEY = \"-McF0sSsk\"  #@param {type:\"string\"}\n",
        "os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY\n",
        "genai.configure(api_key=os.environ[\"GOOGLE_API_KEY\"])\n",
        "\n",
        "model = genai.GenerativeModel('gemini-1.5-pro-latest')  # Or another suitable Gemini model\n",
        "\n",
        "def load_text(file_path):\n",
        "    with Path(file_path).open(\"r\") as file:\n",
        "        return file.read()\n",
        "\n",
        "def save_to_file(responses, output_file):\n",
        "    with Path(output_file).open('w') as file:\n",
        "        file.write(\"\\n\".join(responses))\n",
        "\n",
        "def call_gemini_api(chunk):\n",
        "    \"\"\"Calls the Gemini API to clean the given chunk of text.\"\"\"\n",
        "    prompt = f\"\"\"Clean the following transcripts of all grammatical mistakes, misplaced words, and identify the speakers:\n",
        "    {chunk}\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        response = model.generate_content(prompt)\n",
        "        response.resolve() # Ensure the response is ready\n",
        "        return response.text.strip()\n",
        "    except Exception as e:\n",
        "        print(f\"Error during Gemini API call: {e}\")\n",
        "        return \"\"  # Handle errors gracefully\n",
        "\n",
        "def split_into_chunks(text, n_tokens=300):\n",
        "    encoding = tiktoken.encoding_for_model('gpt-3.5-turbo')  # Using gpt-3.5-turbo tokenizer for token estimation\n",
        "    tokens = encoding.encode(text)\n",
        "    chunks = []\n",
        "    for i in range(0, len(tokens), n_tokens):\n",
        "        chunk_tokens = tokens[i:i + n_tokens]\n",
        "        try:\n",
        "            chunks.append(encoding.decode(chunk_tokens))\n",
        "        except Exception as e:\n",
        "            print(f\"Error decoding tokens: {e}\")\n",
        "            # Handle the error, perhaps by skipping this chunk or using a different decoding strategy\n",
        "            continue\n",
        "    return chunks\n",
        "\n",
        "def process_chunks(input_file, output_file, delay=0):  # delay in seconds (if you hit a rate limit error)\n",
        "    text = load_text(input_file)\n",
        "    chunks = split_into_chunks(text)[:5]\n",
        "    responses = []\n",
        "    for chunk in tqdm(chunks):\n",
        "        responses.append(call_gemini_api(chunk))\n",
        "\n",
        "    save_to_file(responses, output_file)\n",
        "\n",
        "    # Specify your input and output files\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    input_file = \"YouTube.txt\"\n",
        "    output_file = \"clean_transcript.txt\"\n",
        "    process_chunks(input_file, output_file)\n",
        "\n",
        "## Building the Knowledge Base\n",
        "\n",
        "# Read the .txt file into Python\n",
        "with Path('clean_transcript.txt').open('r') as file:\n",
        "    lines = file.read().splitlines()\n",
        "\n",
        "# Group the lines into chunks of 5\n",
        "chunks = [' '.join(lines[i:i+5]) for i in range(0, len(lines), 5)]\n",
        "\n",
        "### Initialize the Embedding Model and Vector DB\n",
        "\n",
        "#No longer use OpenAI Embeddings. We pass transcripts directly to the model.\n",
        "\n",
        "### Initialize the Conversational Agent\n",
        "\n",
        "# conversational memory. No longer using vector store\n",
        "conversational_memory = []\n",
        "\n",
        "def clean_query(query):\n",
        "    prompt = f\"\"\"Clean the following query to make sure it is gramatically correct and can retrieve the correct information: {query}\"\"\"\n",
        "    try:\n",
        "        response = model.generate_content(prompt)\n",
        "        response.resolve() # Ensure the response is ready\n",
        "        return response.text.strip()\n",
        "    except Exception as e:\n",
        "        print(f\"Error during Gemini API call: {e}\")\n",
        "        return \"\"  # Handle errors gracefully\n",
        "\n",
        "\n",
        "def query_knowledge_base(query, knowledge_base):\n",
        "    \"\"\"\n",
        "    Queries the knowledge base using the Gemini API.\n",
        "    \"\"\"\n",
        "    context = \"\\n\".join(knowledge_base)\n",
        "    prompt = f\"\"\"You are a chatbot that answers question to the best of your ability using the context provided\n",
        "    Context:\n",
        "    {context}\n",
        "\n",
        "    Question: {query}\n",
        "    \"\"\"\n",
        "    try:\n",
        "        response = model.generate_content(prompt)\n",
        "        response.resolve() # Ensure the response is ready\n",
        "        return response.text.strip()\n",
        "    except Exception as e:\n",
        "        print(f\"Error during Gemini API call: {e}\")\n",
        "        return \"\"  # Handle errors gracefully\n",
        "\n",
        "def conversational_agent(query, knowledge_base, chat_history):\n",
        "    \"\"\"\n",
        "    A conversational agent that uses the Gemini API to answer questions.\n",
        "    \"\"\"\n",
        "\n",
        "    # Clean query before processing\n",
        "    cleaned_query = clean_query(query)\n",
        "\n",
        "    # Query the knowledge base\n",
        "    response = query_knowledge_base(cleaned_query, knowledge_base)\n",
        "\n",
        "    # Update the chat history\n",
        "    chat_history.append(f\"User: {query}\")\n",
        "    chat_history.append(f\"Assistant: {response}\")\n",
        "\n",
        "    return response, chat_history\n",
        "\n",
        "### Using the Conversational Agent\n",
        "\n",
        "# Example usage\n",
        "knowledge_base = chunks.copy() #copy chunks text snippets to knowledge base\n",
        "chat_history = [] #Initialize empty chat history\n",
        "\n",
        "query1 = \"Tell me about yourself\"\n",
        "response1, chat_history = conversational_agent(query1, knowledge_base, chat_history)\n",
        "print(response1)\n",
        "\n",
        "query2 = \"What are some strategies for growing an audience?\"\n",
        "response2, chat_history = conversational_agent(query2, knowledge_base, chat_history)\n",
        "print(response2)\n",
        "\n",
        "query3 = \"Can you summarize what we've talked about?\"\n",
        "response3, chat_history = conversational_agent(query3, knowledge_base, chat_history)\n",
        "print(response3)\n",
        "\n",
        "print(chat_history)\n",
        "\n",
        "# Once finished, delete the Pinecone index to save resources:\n",
        "\n",
        "# pinecone.delete_index(index_name) #No longer using vector database"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "rWtJ0WEGkavc"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.12"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "9eea814462bb48f2ac468e649311019b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_53bfc84e8a4c43f98095e6dcc543f85a",
              "IPY_MODEL_4ea0d00cccae46ebbe5043d5910667e9",
              "IPY_MODEL_ca5c4ed971e94b4aa0dd8b8abc61363a"
            ],
            "layout": "IPY_MODEL_985bf8b6b856453eb49218880723344b"
          }
        },
        "53bfc84e8a4c43f98095e6dcc543f85a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0c7f76f9ecce49baa310f1acff5450a9",
            "placeholder": "​",
            "style": "IPY_MODEL_ac9aff5b622848f99528d8b1ef39d1f7",
            "value": "100%"
          }
        },
        "4ea0d00cccae46ebbe5043d5910667e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5604993df40848f49b4f39c2f9e3855c",
            "max": 9,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8c169a1c70854b07bbcbe5fcbb508429",
            "value": 9
          }
        },
        "ca5c4ed971e94b4aa0dd8b8abc61363a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_000f3fb5bce64e5980d53a64ec814d73",
            "placeholder": "​",
            "style": "IPY_MODEL_ee17cdf174f440459ab0dfdd20e109b3",
            "value": " 9/9 [00:15&lt;00:00,  1.30s/it]"
          }
        },
        "985bf8b6b856453eb49218880723344b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0c7f76f9ecce49baa310f1acff5450a9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ac9aff5b622848f99528d8b1ef39d1f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5604993df40848f49b4f39c2f9e3855c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8c169a1c70854b07bbcbe5fcbb508429": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "000f3fb5bce64e5980d53a64ec814d73": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ee17cdf174f440459ab0dfdd20e109b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}